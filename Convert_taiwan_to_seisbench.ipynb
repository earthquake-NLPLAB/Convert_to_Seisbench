{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ec1d2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pymysql\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import h5py\n",
    "import math\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy.clients.iris import Client\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy import integrate\n",
    "from scipy.signal import sosfilt, iirfilter, zpk2sos\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "reverse-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "wanted-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_valid(wavefile):\n",
    "    sql = 'SELECT d.label, d.s_time_available\\\n",
    "            FROM Event_Station_Data_new d \\\n",
    "            WHERE d.wave_file=\\'' + str(wavefile) + '\\';'\n",
    "    cursor.execute(sql)\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    if results[-1] == 0:\n",
    "        if '1' in results[0]:\n",
    "            return '1'\n",
    "        else:\n",
    "            return '0'\n",
    "        \n",
    "    label = results[0][0].split(',')\n",
    "\n",
    "    # label 個數為偶數次，取最後 label 結果為最終結果\n",
    "    zero = label.count('0')\n",
    "    one = label.count('1')\n",
    "\n",
    "    if zero > one:\n",
    "        return '0'\n",
    "    else:\n",
    "        return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "005fe703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(p, s, h5path, number_of_event, station_list, sensor, usage, record_inst):\n",
    "    all_df = pd.DataFrame()\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    client = Client()\n",
    "    \n",
    "    hf = h5py.File(h5path, 'a')\n",
    "    \n",
    "    # source attributes\n",
    "    df['source_origin_time'] = [p['ori_time']]\n",
    "    df['source_depth_km'] = [p['depth']]\n",
    "    df['source_latitude_deg'] = [p['lat']]\n",
    "    df['source_longitude_deg'] = [p['lon']]\n",
    "    df['source_magnitude'] = [p['mag']]\n",
    "    df['source_magnitude_type'] = ['ml']    # 芮氏規模\n",
    "    df['source_gap_deg'] = [p['gap']]\n",
    "    \n",
    "#     for k in tqdm(p.keys(), total=len(list(p.keys()))):\n",
    "    for k in p.keys():\n",
    "        try:\n",
    "            if 'distance' in p[k].keys():\n",
    "                n_data = p[k]['numberOfData']\n",
    "                \n",
    "                for n in range(n_data):\n",
    "                    instrument = p[k][str(n)]['instrument']\n",
    "                    status = check_valid(p[k][str(n)]['waveFile'])\n",
    "                    \n",
    "                    df['station_code'] = [k]\n",
    "                    \n",
    "                    # 新格式的 network = TW\n",
    "                    df['station_network_code'] = ['TW']\n",
    "                    \n",
    "                    # 新格式的 location = 10, 00, 20\n",
    "                    df['station_location_code'] = [convert_to_new_location(p[k][str(n)]['location'])]\n",
    "                    df['station_p_polar'] = p[k]['p_polar']\n",
    "                    \n",
    "                    # 特別標示出這個 trace 有多個地震事件\n",
    "                    df['trace_event_number'] = [number_of_event]\n",
    "                    df['trace_channel'] = [convert_instrument_to_channel(instrument)]\n",
    "                    \n",
    "                    df['trace_start_time'] = [p[k][str(n)]['starttime']]\n",
    "                    \n",
    "                    factor, lon, lat, ele = get_factor(k, instrument, p[k][str(n)]['network'], p[k][str(n)]['location'], \n",
    "                                                       p[k][str(n)]['starttime'], s, station_list, sensor, usage, record_inst)\n",
    "                    \n",
    "                    df['trace_factor_ZNE'], df['station_longitude_deg'], df['station_latitude_deg'], df['station_elevation_m'] = [factor], [lon], [lat], [ele]\n",
    "                    \n",
    "                    # 若有多的事件在同個波型，則對應到相同的 trace\n",
    "                    df['trace_name'] = [p['event'][:-1] + '_' + k + '_' + df['trace_channel'].iloc[0] + '_' + \\\n",
    "                                        p[k][str(n)]['network'] + '_' + df['station_location_code'].iloc[0]]\n",
    "                    \n",
    "                    sampling_rate = p[k][str(n)]['sampling_rate']\n",
    "                    df['trace_sampling_rate_hz'] = [sampling_rate]\n",
    "                    \n",
    "                    df['trace_npts'] = [time_diff_sample(p[k][str(n)]['starttime'], p[k][str(n)]['endtime'], sampling_rate)]\n",
    "                    df['trace_p_arrival_sample'] = [time_diff_sample(p[k][str(n)]['starttime'], p[k][str(n)]['p_arrival_time'], sampling_rate)]\n",
    "                    df['trace_p_status'] = ['maunal']\n",
    "\n",
    "                    if p[k][str(n)]['DataAvailable']['Stime']:\n",
    "                        df['trace_s_arrival_sample'] = [time_diff_sample(p[k][str(n)]['starttime'], p[k][str(n)]['s_arrival_time'], sampling_rate)]\n",
    "                        df['path_p_travel_s'] = [(df['trace_s_arrival_sample'].iloc[0]-df['trace_p_arrival_sample'].iloc[0]) / sampling_rate ]\n",
    "                        df['trace_s_status'] = ['maunal']\n",
    "                        df['trace_s_weight'] = [p[k]['s_weight']]\n",
    "                    else:\n",
    "                        df['trace_s_arrival_sample'] = [None]\n",
    "                        df['trace_s_status'] = [None]\n",
    "                        df['path_p_travel_s'] = [None]\n",
    "                        df['trace_s_weight'] = [None]\n",
    "                    \n",
    "                    df['trace_p_weight'] = [p[k]['p_weight']]\n",
    "                  \n",
    "                    df['path_ep_distance_km'] = [p[k]['distance']]\n",
    "                    df['path_p_residual_s'] = [p[k]['p_residual']]\n",
    "                    df['path_s_residual_s'] = [p[k]['s_residual']]\n",
    "                    df['path_back_azimuth_deg'] = [client.distaz(stalat=df['station_latitude_deg'].iloc[0], stalon=df['station_longitude_deg'].iloc[0], \n",
    "                                                                evtlat=df['source_latitude_deg'].iloc[0], evtlon=df['source_longitude_deg'].iloc[0])['backazimuth']]\n",
    "                    \n",
    "                    if df['path_p_travel_s'].iloc[0]  > 0:\n",
    "                        df['trace_category'] = ['earthquake']\n",
    "                    \n",
    "                    if status == '1':\n",
    "                        df['trace_completeness'] = [1]\n",
    "                        wave_z, wave_n, wave_e = readWave('/mnt/nas2/CWBSN_modify_time/'+p[k][str(n)]['waveFile'])\n",
    "                        \n",
    "                        df['trace_snr_db'] = [round(snr_in_db(wave_z, wave_n, wave_e, df['trace_p_arrival_sample'].iloc[0]), 3)]\n",
    "                        df['trace_Z_snr_db'] = [round(Zsnr_in_db(wave_z, df['trace_p_arrival_sample'].iloc[0]), 3)]\n",
    "                    \n",
    "                        pga, pga_g, z_pga = calc_pga(wave_z, wave_n, wave_e, sampling_rate, instrument)\n",
    "                        pgv, z_pgv = calc_pgv(wave_z, wave_n, wave_e, sampling_rate, instrument)\n",
    "      \n",
    "                        df['trace_pga_perg'] = [round(pga_g, 3)]\n",
    "                        df['trace_pga_cmps2'] = [round(pga, 3)]\n",
    "                        df['trace_Z_pga_cmps2'] = [round(z_pga, 3)]\n",
    "                        df['trace_pgv_cmps'] = [round(pgv, 3)]\n",
    "                        df['trace_Z_pgv_cmps'] = [round(z_pgv, 3)]\n",
    "                        \n",
    "                        # write trace into hdf5 file: (3, npts)\n",
    "                        wave = np.vstack((wave_z, wave_n, wave_e))\n",
    "                       \n",
    "                        hf.create_dataset('data/'+df['trace_name'].iloc[0], wave.shape, data=wave)\n",
    "                    else:\n",
    "                        df['trace_completeness'] = [0]\n",
    "                    \n",
    "                    all_df = pd.concat([all_df, df])\n",
    "                    \n",
    "        except Exception as exx:\n",
    "#             print(exx)\n",
    "            pass\n",
    "        \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "mechanical-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfile(base_dir, year):\n",
    "    files = glob.glob(base_dir+year+'/*.json')\n",
    "    \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-ideal",
   "metadata": {},
   "source": [
    "### Start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "numerous-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/nas2/CWBSN_modify_time/\"\n",
    "# year = [str(y) for y in range(2012, 2013)]\n",
    "year = ['2020']\n",
    "s = get_StationInfo()\n",
    "station_list = pd.read_excel(\"/mnt/nas4/weiwei/seisbench_CWBSN/Station.xlsx\")\n",
    "sensor = pd.read_excel(\"/mnt/nas4/weiwei/seisbench_CWBSN/Sensor.xlsx\")\n",
    "usage = pd.read_excel(\"/mnt/nas4/weiwei/seisbench_CWBSN/Usage.xlsx\")\n",
    "record_inst = 'CWBSN'\n",
    "\n",
    "db_setting = {\n",
    "    'host': '127.0.0.1', \n",
    "    'port': 3300,\n",
    "    'user': 'root',\n",
    "    'password': 'earthquake_123456',\n",
    "    'database': 'earthquake',\n",
    "    'passwd': 'earthquake_123456',\n",
    "    \"charset\": \"utf8\"\n",
    "}\n",
    "db = pymysql.connect(**db_setting)\n",
    "cursor = db.cursor()\n",
    "\n",
    "format = {\n",
    "    'dimension_order': 'CW',\n",
    "    'component_order': 'ZNE',\n",
    "    'sampling_rate': 100,\n",
    "    'unit': 'cmps/cmps2',\n",
    "    'measurement': 'velocity/acceleration'\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/mnt/nas2/CWBSN_modify_time/seisbench\"):\n",
    "    os.makedirs(\"/mnt/nas2/CWBSN_modify_time/seisbench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-century",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year:  2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▍                                                                                                                                         | 39/509 [5:02:17<59:44:01, 457.54s/it]"
     ]
    }
   ],
   "source": [
    "def main(y)\n",
    "    df = pd.DataFrame()\n",
    "    print('year: ', y)\n",
    "    \n",
    "    files = getfile(base_dir, y)\n",
    "    \n",
    "    for f in tqdm(files, total=len(files)):\n",
    "        number_of_event = check_multiple_event(f)\n",
    "        \n",
    "        json_file = open(f)\n",
    "        p = json.load(json_file)\n",
    "        \n",
    "        json_df = parse_file(p, s, '/mnt/nas2/CWBSN_modify_time/seisbench/chunks_' + y + '.hdf5', number_of_event,\n",
    "                            station_list, sensor, usage, record_inst)\n",
    "            \n",
    "        df = pd.concat([df, json_df])\n",
    "        \n",
    "    # write into files: metadata\n",
    "    df.to_csv('/mnt/nas2/CWBSN_modify_time/seisbench/metadata_' + y + '.csv')\n",
    "    \n",
    "    # write into hdf5: chunks/data_format\n",
    "    hf = h5py.File('/mnt/nas2/CWBSN_modify_time/seisbench/chunks_' + y + '.hdf5', 'a')\n",
    "    for k, v in format.items():\n",
    "        hf.create_dataset('data_format/'+k, data=v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2012, 2022):\n",
    "    p = Process(target=main, args=([str(i)]))\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"/mnt/nas2/CWBSN_modify_time/seisbench/chunks_2019.hdf5\", 'r')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "combined-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(f['data'].keys())\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-helping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earthquake",
   "language": "python",
   "name": "earthquake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
